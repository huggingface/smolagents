# ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ğŸ¤–ğŸ¤ğŸ¤–

[[Colabì—ì„œ ì—´ê¸°]]

ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” **ë©€í‹° ì—ì´ì „íŠ¸ ì›¹ ë¸Œë¼ìš°ì €**ë¥¼ ë§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤. ì´ëŠ” ì›¹ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ ì—ì´ì „íŠ¸ê°€ í˜‘ë ¥í•˜ëŠ” ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì…ë‹ˆë‹¤!

ë©€í‹° ì—ì´ì „íŠ¸ëŠ” ê°„ë‹¨í•œ ê³„ì¸µ êµ¬ì¡°ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

```
              +----------------+
              | Manager agent  |
              +----------------+
                       |
        _______________|______________
       |                              |
Code Interpreter            +------------------+
    tool                    | Web Search agent |
                            +------------------+
                               |            |
                        Web Search tool     |
                                   Visit webpage tool
```
ì´ ì‹œìŠ¤í…œì„ ì„¤ì •í•´ë³´ê² ìŠµë‹ˆë‹¤. 

ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì—¬ í•„ìš”í•œ ì¢…ì†ì„±ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤.

```py
!pip install smolagents[toolkit] --upgrade -q
```

Inference Providersë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ Hugging Faceì— ë¡œê·¸ì¸í•©ë‹ˆë‹¤:

```py
from huggingface_hub import login

login()
```

âš¡ï¸ ì €í¬ ì—ì´ì „íŠ¸ëŠ” HFì˜ Inference APIë¥¼ ì‚¬ìš©í•˜ëŠ” `InferenceClientModel` í´ë˜ìŠ¤ë¥¼ í†µí•´ [Qwen/Qwen2.5-Coder-32B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct)ë¡œ êµ¬ë™ë©ë‹ˆë‹¤. Inference APIë¥¼ ì‚¬ìš©í•˜ë©´ ëª¨ë“  OS ëª¨ë¸ì„ ë¹ ë¥´ê³  ì‰½ê²Œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

> [!TIP]
> Inference ProvidersëŠ” ì„œë²„ë¦¬ìŠ¤ ì¶”ë¡  íŒŒíŠ¸ë„ˆê°€ ì§€ì›í•˜ëŠ” ìˆ˜ë°± ê°œì˜ ëª¨ë¸ì— ëŒ€í•œ ì•¡ì„¸ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì§€ì›ë˜ëŠ” í”„ë¡œë°”ì´ë” ëª©ë¡ì€ [ì—¬ê¸°](https://huggingface.co/docs/inference-providers/index)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```py
model_id = "Qwen/Qwen2.5-Coder-32B-Instruct"
```

## ğŸ” ì›¹ ê²€ìƒ‰ ë„êµ¬ ìƒì„±

ì›¹ ë¸Œë¼ìš°ì§•ì„ ìœ„í•´ Google ê²€ìƒ‰ê³¼ ë™ë“±í•œ ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” ê¸°ë³¸ [`WebSearchTool`] ë„êµ¬ë¥¼ ì´ë¯¸ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

í•˜ì§€ë§Œ `WebSearchTool`ì—ì„œ ì°¾ì€ í˜ì´ì§€ë¥¼ í™•ì¸í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ë„ í•„ìš”í•©ë‹ˆë‹¤.
ì´ë¥¼ ìœ„í•´ ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ë‚´ì¥ëœ `VisitWebpageTool`ì„ ì‚¬ìš©í•  ìˆ˜ë„ ìˆì§€ë§Œ, ì‘ë™ ì›ë¦¬ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ ì§ì ‘ êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤.

ê·¸ë˜ì„œ `markdownify`ë¥¼ ì‚¬ìš©í•˜ì—¬ `VisitWebpageTool` ë„êµ¬ë¥¼ ì²˜ìŒë¶€í„° ë§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤.

```py
import re
import requests
from markdownify import markdownify
from requests.exceptions import RequestException
from smolagents import tool


@tool
def visit_webpage(url: str) -> str:
    """ì£¼ì–´ì§„ URLì˜ ì›¹í˜ì´ì§€ì— ì ‘ì†í•˜ì—¬ ê·¸ ë‚´ìš©ì„ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ë°˜í™˜í•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
        url: ë°©ë¬¸í•  ì›¹í˜ì´ì§€ì˜ URL.

    ë°˜í™˜ê°’:
        ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜ëœ ì›¹í˜ì´ì§€ ë‚´ìš©, ë˜ëŠ” ìš”ì²­ì´ ì‹¤íŒ¨í•  ê²½ìš° ì˜¤ë¥˜ ë©”ì‹œì§€.
    """
    try:
        # URLì— GET ìš”ì²­ ì „ì†¡
        response = requests.get(url)
        response.raise_for_status()  # ì˜ëª»ëœ ìƒíƒœ ì½”ë“œì— ëŒ€í•´ ì˜ˆì™¸ ë°œìƒ

        # HTML ë‚´ìš©ì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜
        markdown_content = markdownify(response.text).strip()

        # ì—¬ëŸ¬ ì¤„ ë°”ê¿ˆ ì œê±°
        markdown_content = re.sub(r"\n{3,}", "\n\n", markdown_content)

        return markdown_content

    except RequestException as e:
        return f"Error fetching the webpage: {str(e)}"
    except Exception as e:
        return f"An unexpected error occurred: {str(e)}"
```

ì´ì œ ë„êµ¬ë¥¼ ì´ˆê¸°í™”í•˜ê³  í…ŒìŠ¤íŠ¸í•´ë³´ê² ìŠµë‹ˆë‹¤!

```py
print(visit_webpage("https://en.wikipedia.org/wiki/Hugging_Face")[:500])
```

## ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ êµ¬ì¶• ğŸ¤–ğŸ¤ğŸ¤–

ì´ì œ `search`ì™€ `visit_webpage` ë„êµ¬ê°€ ëª¨ë‘ ì¤€ë¹„ë˜ì—ˆìœ¼ë¯€ë¡œ, ì´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ ì—ì´ì „íŠ¸ì— ì–´ë–¤ êµ¬ì„±ì„ ì„ íƒí• ê¹Œìš”?
- ì›¹ ë¸Œë¼ìš°ì§•ì€ ë³‘ë ¬ ë„êµ¬ í˜¸ì¶œì´ í•„ìš”ì—†ëŠ” ë‹¨ì¼ íƒ€ì„ë¼ì¸ ì‘ì—…ì´ë¯€ë¡œ, JSON ë„êµ¬ í˜¸ì¶œ ë°©ì‹ì´ ì í•©í•©ë‹ˆë‹¤. ë”°ë¼ì„œ `ToolCallingAgent`ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.
- ë˜í•œ ì›¹ ê²€ìƒ‰ì€ ì˜¬ë°”ë¥¸ ë‹µì„ ì°¾ê¸° ì „ì— ë§ì€ í˜ì´ì§€ë¥¼ íƒìƒ‰í•´ì•¼ í•˜ëŠ” ê²½ìš°ê°€ ìˆìœ¼ë¯€ë¡œ, `max_steps`ë¥¼ 10ìœ¼ë¡œ ëŠ˜ë¦¬ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

```py
from smolagents import (
    CodeAgent,
    ToolCallingAgent,
    InferenceClientModel,
    WebSearchTool,
    LiteLLMModel,
)

model = InferenceClientModel(model_id=model_id)

web_agent = ToolCallingAgent(
    tools=[WebSearchTool(), visit_webpage],
    model=model,
    max_steps=10,
    name="web_search_agent",
    description="Runs web searches for you.",
)
```

ì´ ì—ì´ì „íŠ¸ì— `name`ê³¼ `description` ì†ì„±ì„ ë¶€ì—¬í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ì´ ì—ì´ì „íŠ¸ê°€ ë§¤ë‹ˆì € ì—ì´ì „íŠ¸ì— ì˜í•´ í˜¸ì¶œë  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” í•„ìˆ˜ ì†ì„±ì…ë‹ˆë‹¤.

ê·¸ ë‹¤ìŒ ë§¤ë‹ˆì € ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•˜ê³ , ì´ˆê¸°í™” ì‹œ `managed_agents` ì¸ìˆ˜ì— ê´€ë¦¬ë˜ëŠ” ì—ì´ì „íŠ¸ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.

ì´ ì—ì´ì „íŠ¸ëŠ” ê³„íšê³¼ ì‚¬ê³ ë¥¼ ë‹´ë‹¹í•˜ë¯€ë¡œ, ê³ ê¸‰ ì¶”ë¡ ì´ ìœ ìš©í•  ê²ƒì…ë‹ˆë‹¤. ë”°ë¼ì„œ `CodeAgent`ê°€ ì˜ ì‘ë™í•  ê²ƒì…ë‹ˆë‹¤.

ë˜í•œ í˜„ì¬ ì—°ë„ë¥¼ í¬í•¨í•˜ê³  ì¶”ê°€ ë°ì´í„° ê³„ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” ì§ˆë¬¸ì„ í•˜ê³  ì‹¶ìœ¼ë¯€ë¡œ, ì—ì´ì „íŠ¸ê°€ ì´ëŸ¬í•œ íŒ¨í‚¤ì§€ë¥¼ í•„ìš”ë¡œ í•  ê²½ìš°ì— ëŒ€ë¹„í•´ `additional_authorized_imports=["time", "numpy", "pandas"]`ë¥¼ ì¶”ê°€í•´ë³´ê² ìŠµë‹ˆë‹¤.

```py
manager_agent = CodeAgent(
    tools=[],
    model=model,
    managed_agents=[web_agent],
    additional_authorized_imports=["time", "numpy", "pandas"],
)
```

ì´ê²Œ ì „ë¶€ì…ë‹ˆë‹¤! ì´ì œ ì‹œìŠ¤í…œì„ ì‹¤í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤! ê³„ì‚°ê³¼ ì—°êµ¬ê°€ ëª¨ë‘ í•„ìš”í•œ ì§ˆë¬¸ì„ ì„ íƒí•©ë‹ˆë‹¤.

```py
answer = manager_agent.run("If LLM training continues to scale up at the current rhythm until 2030, what would be the electric power in GW required to power the biggest training runs by 2030? What would that correspond to, compared to some countries? Please provide a source for any numbers used.")
```

We get this report as the answer:
```
Based on current growth projections and energy consumption estimates, if LLM trainings continue to scale up at the 
current rhythm until 2030:

1. The electric power required to power the biggest training runs by 2030 would be approximately 303.74 GW, which 
translates to about 2,660,762 GWh/year.

2. Comparing this to countries' electricity consumption:
   - It would be equivalent to about 34% of China's total electricity consumption.
   - It would exceed the total electricity consumption of India (184%), Russia (267%), and Japan (291%).
   - It would be nearly 9 times the electricity consumption of countries like Italy or Mexico.

3. Source of numbers:
   - The initial estimate of 5 GW for future LLM training comes from AWS CEO Matt Garman.
   - The growth projection used a CAGR of 79.80% from market research by Springs.
   - Country electricity consumption data is from the U.S. Energy Information Administration, primarily for the year 
2021.
```

[ìŠ¤ì¼€ì¼ë§ ê°€ì„¤](https://gwern.net/scaling-hypothesis)ì´ ê³„ì† ì°¸ì´ë¼ë©´ ìƒë‹¹íˆ í° ë°œì „ì†Œê°€ í•„ìš”í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤.

ì—ì´ì „íŠ¸ë“¤ì´ ì‘ì—…ì„ í•´ê²°í•˜ê¸° ìœ„í•´ íš¨ìœ¨ì ìœ¼ë¡œ í˜‘ë ¥í–ˆìŠµë‹ˆë‹¤! âœ…

ğŸ’¡ ì´ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ì„ ë” ë§ì€ ì—ì´ì „íŠ¸ë¡œ ì‰½ê²Œ í™•ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤: í•˜ë‚˜ëŠ” ì½”ë“œ ì‹¤í–‰ì„, ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ì›¹ ê²€ìƒ‰ì„,  ë˜ ë‹¤ë¥¸ í•˜ë‚˜ëŠ” íŒŒì¼ ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” ì‹ìœ¼ë¡œ...
